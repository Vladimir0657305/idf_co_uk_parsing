# Создаем цикл для перебора всех страниц
for page in range(1, last_page+1):
    # Формируем ссылку на текущую страницу
    url = f'https://www.idf.co.uk/patients/find-a-doctor.aspx?Specialty=20&SubSpecialty=0&AreaCode=W1G&SearchCriteria=London&PageNumber={page}'

    # Загружаем страницу
    driver.get(url)

    # Ожидание загрузки страницы
    wait = WebDriverWait(driver, 10)
    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.section')))

    # Получение HTML-кода страницы с результатами поиска
    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')
    links = soup.select('.docresults a[href]')
    doctor_links = [urljoin(base_url, link['href']) for link in links]

    # Добавляем найденные ссылки на врачей на текущей странице в общий список
    all_doctor_links.extend(doctor_links)

# Создаем цикл для перебора всех ссылок на врачей
for doctor_link in all_doctor_links:
    # Загружаем страницу врача
    driver.get(doctor_link)
    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.content3')))
# Создаем объект BeautifulSoup
html_code = driver.page_source
soup = BeautifulSoup(html_code, 'html.parser')

# Получаем имя и квалификацию
name = soup.select_one('h2.memberprofile').text.strip()
qualifications = soup.select_one('span.qualifications').text.strip()

# Получаем специальность
specialty_label = soup.select_one('span.strong', text='Specialties:')
specialty = specialty_label.find_next_sibling().text.strip()

# Получаем адрес, телефон, почту и сайт
address_label = soup.select_one('span.strong', text='Primary Practice:')
address = address_label.find_next_sibling().strip()
telephone_label = soup.select_one('span.strong', text='Appointments Telephone:')
telephone = telephone_label.find_next_sibling().strip()
email_label = soup.select_one('span.strong', text='Email Address:')
email = email_label.find_next_sibling().select_one('a').text.strip()
website_label = soup.select_one('span.strong', text='Website:')
website = website_label.find_next_sibling().select_one('a')['href']

# Выводим результаты
print(f"Name: {name}")
print(f"Qualifications: {qualifications}")
print(f"Specialty: {specialty}")
print(f"Address: {address}")
print(f"Telephone: {telephone}")
print(f"Email: {email}")
print(f"Website: {website}")

# Закрытие браузера
driver.quit()









































import csv
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Создаем объект драйвера Chrome
driver = webdriver.Chrome()

# Задаем базовый URL
base_url = 'https://www.idf.co.uk'

# Создаем список всех ссылок на врачей
all_doctor_links = []

# Определяем количество страниц
driver.get('https://www.idf.co.uk/patients/find-a-doctor.aspx?Specialty=20&SubSpecialty=0&AreaCode=W1G&SearchCriteria=London')
last_page_link = driver.find_element_by_css_selector('.pages ul li:last-child a')
last_page = int(last_page_link.text)

# Создаем файл CSV для записи данных
with open('doctors.csv', mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['Number', 'Name', 'Qualifications', 'Specialty', 'Address', 'Telephone', 'Email', 'Website'])

    # Создаем цикл для перебора всех страниц
    for page in range(1, last_page+1):
        # Формируем ссылку на текущую страницу
        url = f'https://www.idf.co.uk/patients/find-a-doctor.aspx?Specialty=20&SubSpecialty=0&AreaCode=W1G&SearchCriteria=London&PageNumber={page}'

        # Загружаем страницу
        driver.get(url)

        # Ожидание загрузки страницы
        wait = WebDriverWait(driver, 10)
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.section')))

        # Получение HTML-кода страницы с результатами поиска
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        links = soup.select('.docresults a[href]')
        doctor_links = [urljoin(base_url, link['href']) for link in links]

        # Добавляем найденные ссылки на врачей на текущей странице в общий список
        all_doctor_links.extend(doctor_links)

    # Создаем глобальный счетчик
    counter = 0

    # Создаем цикл для перебора всех ссылок на врачей
    for doctor_link in all_doctor_links:
        # Загружаем страницу врача
        driver.get(doctor_link)
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.content3')))

        # Создаем объект BeautifulSoup
        html_code = driver.page_source
        soup = BeautifulSoup(html_code, 'html.parser')

        # Получаем имя и квалификацию
        name = soup.select_one('h2.memberprofile').text.strip()
        qualifications = soup.select_one('span.qualifications').text.strip()

